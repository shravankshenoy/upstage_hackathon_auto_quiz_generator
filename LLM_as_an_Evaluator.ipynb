{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDK2nk8o4cb7",
        "outputId": "239693b8-90ee-482d-e331-ee3a2007ea12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m391.5/391.5 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.4/140.4 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.4/362.4 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "! pip3 install -qU  markdownify  langchain-upstage rank_bm25 python-dotenv\n",
        "! pip install langchain-chroma\n",
        "! pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "enZA9AD74nw5"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "upstage_api_key = getpass.getpass(\"Enter your Upstage API key: \")\n",
        "os.environ.setdefault('UPSTAGE_API_KEY', upstage_api_key)"
      ],
      "metadata": {
        "id": "Z8aK1rCS_ajE"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iNi5wtzN_1hF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_upstage import UpstageLayoutAnalysisLoader\n",
        "\n",
        "\n",
        "layzer = UpstageLayoutAnalysisLoader(\"pilot_JEJU_ENG.pdf\", output_type=\"html\")\n",
        "# For improved memory efficiency, consider using the lazy_load method to load documents page by page.\n",
        "docs = layzer.load()  # or layzer.lazy_load()"
      ],
      "metadata": {
        "id": "TpG5ODE14xA1"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jh5DRO6I_20b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We take the output of LLM 1 which is shown below and pass it to LLM 2"
      ],
      "metadata": {
        "id": "ivB4QD-4-2jU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## LLM 1 Output\n",
        "questions = [\n",
        "    {'question': 'What is the main aim of the Jeju government in terms of Jeju culture?',\n",
        "     'options': ['a. To promote modernization', 'b. To preserve traditional culture', 'c. To merge indigenous and new cultures', 'd. To focus on tangible heritages'],\n",
        "     'right_option': 'b. To preserve traditional culture'\n",
        "    },\n",
        "    {'question': 'What is the biggest challenge in the Culture Pilot Cities programme?',\n",
        "     'options': ['a. Determining the scope of the project', 'b. Establishing good practices relevant to Agenda 21', 'c. Understanding Jeju culture from a short visit', 'd. Predicting the difference between viewpoints'],\n",
        "     'right_option': 'a. Determining the scope of the project'\n",
        "    }\n",
        "]\n"
      ],
      "metadata": {
        "id": "diRIvN5O6FFM"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import (\n",
        "    Language,\n",
        "    RecursiveCharacterTextSplitter,\n",
        ")\n",
        "\n",
        "# 2. Split\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_language(\n",
        "    chunk_size=1000, chunk_overlap=100, language=Language.HTML\n",
        ")\n",
        "splits = text_splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "HFBXN5QE5SiM"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TfEeM2LF6xt_"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_chroma import Chroma\n",
        "\n",
        "# 3. Embed & indexing\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=splits,\n",
        "    embedding=UpstageEmbeddings(model=\"solar-embedding-1-large\"),\n",
        ")"
      ],
      "metadata": {
        "id": "gO-lhrpt5YXC"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_upstage import ChatUpstage\n",
        "\n",
        "\n",
        "llm = ChatUpstage()\n",
        "\n",
        "prompt_template = PromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "    Please provide most correct answer from the following context.\n",
        "    In the answer, just specify the letter corresponding to the option.\n",
        "    If you don't know the answer, just provide your best guess and do not provide any reasoning.\n",
        "\n",
        "    For example, if you think the answer is the first option, just write \"a\".\n",
        "    If you think the answer is the second option, just write \"b\", and so on.\n",
        "\n",
        "    ---\n",
        "    Question: {question}\n",
        "    ---\n",
        "    Options: {options}\n",
        "    ---\n",
        "    Context: {context}\n",
        "\n",
        "\n",
        "    chosen option (a, b, c, or d):\n",
        "\n",
        "    \"\"\"\n",
        ")\n",
        "chain = prompt_template | llm | StrOutputParser()"
      ],
      "metadata": {
        "id": "chrup8tJ6MI1"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectorstore.as_retriever()\n",
        "result_docs = retriever.invoke(questions[0][\"question\"])\n",
        "# Answer from LLM 2\n",
        "chain.invoke({\"question\": questions[0][\"question\"], \"options\":questions[0][\"options\"] ,\"context\": result_docs})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "mbZDjJmo7Nwl",
        "outputId": "da550fd7-ab04-4a1e-c595-02ded461d32b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'b. To preserve traditional culture'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Answer from LLM 1\n",
        "questions[0]['right_option']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "6sAl6KY_7XyD",
        "outputId": "6699ae11-209c-46b0-b134-62f060df7718"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'b. To preserve traditional culture'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Both LLM 1 and LLM 2 are giving the same answer as output, hence the question is marked as a valid question"
      ],
      "metadata": {
        "id": "ZqD3RAk_-m5m"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xGXQRSpE-rJ4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}